Scoring a random baseline for task 1
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task1/English/Task1-English-1st-Presidential.txt
INFO : Reading predicted ranking order from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_random_baseline.txt
INFO : ======================================== RESULTS for task1_random_baseline.txt =========================================
INFO : R-PRECISION (R=37):           0.0000    
INFO : ========================================================================================================================
INFO : AVERAGE PRECISION@N:          @1        @3        @5        @10       @20       @50       AP       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0242    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK@N:            @1        @3        @5        @10       @20       @50       RR       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0932    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MAP !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision@N is precision, estimated at each relevant line_number, averaged for all relevant line_numbers up to the N-th (or by the threshold, if it is smaller).
INFO : Reciprocal Rank@N is the sum of the reciprocal ranks of the relevant line_numbers (up to the N-th), according to the ranked list.
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring the gold predictions for task 1
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task1/English/Task1-English-1st-Presidential.txt
INFO : Reading predicted ranking order from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_gold.txt
INFO : ============================================== RESULTS for task1_gold.txt ==============================================
INFO : R-PRECISION (R=37):           1.0000    
INFO : ========================================================================================================================
INFO : AVERAGE PRECISION@N:          @1        @3        @5        @10       @20       @50       AP       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    1.0000    1.0000    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK@N:            @1        @3        @5        @10       @20       @50       RR       
INFO :                               1.0000    1.8333    2.2833    2.9290    3.5977    4.2016    4.2016    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    0.7400    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MAP !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision@N is precision, estimated at each relevant line_number, averaged for all relevant line_numbers up to the N-th (or by the threshold, if it is smaller).
INFO : Reciprocal Rank@N is the sum of the reciprocal ranks of the relevant line_numbers (up to the N-th), according to the ranked list.
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring BOTH the gold and random baseline for task 1 (only for example purposes, scoring multiple files for 1 debate will lead to wrong metrics)
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task1/English/Task1-English-1st-Presidential.txt
INFO : Reading predicted ranking order from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_gold.txt
INFO : ============================================== RESULTS for task1_gold.txt ==============================================
INFO : R-PRECISION (R=37):           1.0000    
INFO : ========================================================================================================================
INFO : AVERAGE PRECISION@N:          @1        @3        @5        @10       @20       @50       AP       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    1.0000    1.0000    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK@N:            @1        @3        @5        @10       @20       @50       RR       
INFO :                               1.0000    1.8333    2.2833    2.9290    3.5977    4.2016    4.2016    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    0.7400    
INFO : ========================================================================================================================
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task1/English/Task1-English-1st-Presidential.txt
INFO : Reading predicted ranking order from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_random_baseline.txt
INFO : ======================================== RESULTS for task1_random_baseline.txt =========================================
INFO : R-PRECISION (R=37):           0.0000    
INFO : ========================================================================================================================
INFO : AVERAGE PRECISION@N:          @1        @3        @5        @10       @20       @50       AP       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0242    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK@N:            @1        @3        @5        @10       @20       @50       RR       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0932    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    
INFO : ========================================================================================================================
INFO : =================================================== AVERAGED RESULTS ===================================================
INFO : MEAN R-PRECISION:             0.5000    
INFO : ========================================================================================================================
INFO : MEAN AVERAGE PRECISION@N:     @1        @3        @5        @10       @20       @50       MAP      
INFO :                               0.5000    0.5000    0.5000    0.5000    0.5000    0.5000    0.5121    
INFO : ========================================================================================================================
INFO : MEAN RECIPROCAL RANK@N:       @1        @3        @5        @10       @20       @50       MRR      
INFO :                               0.5000    0.9167    1.1417    1.4645    1.7989    2.1008    2.1474    
INFO : ========================================================================================================================
INFO : MEAN PRECISION@N:             @1        @3        @5        @10       @20       @50       
INFO :                               0.5000    0.5000    0.5000    0.5000    0.5000    0.3700    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MAP !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision@N is precision, estimated at each relevant line_number, averaged for all relevant line_numbers up to the N-th (or by the threshold, if it is smaller).
INFO : Reciprocal Rank@N is the sum of the reciprocal ranks of the relevant line_numbers (up to the N-th), according to the ranked list.
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
TEST ERROR: Scoring task 1, where the provided list of line_numbers contains a line_number, which is not in the gold file.
ERROR : Problem with line_number: 1500. They should be consecutive and starting from 1.
ERROR : Bad format for pred file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_other_line_number.txt. Cannot score.
**********
TEST ERROR: Scoring task 1, where the provided list of line_numbers does not contain all lines from the gold file.
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task1/English/Task1-English-1st-Presidential.txt
INFO : Reading predicted ranking order from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task1_not_all_lines.txt
ERROR : The predictions do not match the lines from the gold file - missing or extra line_no
Traceback (most recent call last):
  File "task1.py", line 206, in <module>
    thresholds, precisions, avg_precisions, reciprocal_ranks, num_relevant = evaluate(gold_file, pred_file)
  File "task1.py", line 107, in evaluate
    gold_labels, line_score = _read_gold_and_pred(gold_fpath, pred_fpath)
  File "task1.py", line 47, in _read_gold_and_pred
    raise ValueError('The predictions do not match the lines from the gold file - missing or extra line_no')
ValueError: The predictions do not match the lines from the gold file - missing or extra line_no
**********
**********
Scoring the gold predictions for task 2
INFO : Started evaluating results for Task 2 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_gold.txt
INFO : ======================================================= RESULTS ========================================================
INFO : MEAN ABSOLUTE ERROR:          0.0000
INFO : ========================================================================================================================
INFO : MACRO MEAN ABSOLUTE ERROR:    0.0000
INFO : ========================================================================================================================
INFO : ACCURACY:                     1.0000
INFO : ========================================================================================================================
INFO : MACRO F1:                     1.0000
INFO : ========================================================================================================================
INFO : MACRO RECALL:                 1.0000
INFO : ========================================================================================================================
INFO : CONFUSION MATRIX:             
INFO :                      true          false      half-true
INFO : true                    8              0              0
INFO : false                   0             13              0
INFO : half-true               0              0              9
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN ABSOLUTE ERROR !!!
INFO : Mean Absolute Error (MAE) computes the mean "distance" between the predicted and gold labels.
INFO :   For correct predictions the distance is 0.
INFO :   For mistakes between FALSE and TRUE classes it is 2, and for all other mistakes it is 1.
INFO : Macro MAE computes MAE for each of the (gold) classes and takes the average.
INFO : Accuracy computes the percentage of correctly predicted classes.
INFO : Macro F1 computes the F1 score for each of the classes and takes their average.
INFO : Macro Recall computes Recall for each of the classes and takes its average.
INFO : Confusion Matrix computes the distribution of predicted classes, where rows are true labels and columns are predicted ones.
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring a random baseline for task 2
INFO : Started evaluating results for Task 2 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_random_baseline.txt
INFO : ======================================================= RESULTS ========================================================
INFO : MEAN ABSOLUTE ERROR:          0.5333
INFO : ========================================================================================================================
INFO : MACRO MEAN ABSOLUTE ERROR:    0.5132
INFO : ========================================================================================================================
INFO : ACCURACY:                     0.6000
INFO : ========================================================================================================================
INFO : MACRO F1:                     0.5912
INFO : ========================================================================================================================
INFO : MACRO RECALL:                 0.6054
INFO : ========================================================================================================================
INFO : CONFUSION MATRIX:             
INFO :                      true          false      half-true
INFO : true                    4              1              3
INFO : false                   3              7              3
INFO : half-true               2              0              7
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN ABSOLUTE ERROR !!!
INFO : Mean Absolute Error (MAE) computes the mean "distance" between the predicted and gold labels.
INFO :   For correct predictions the distance is 0.
INFO :   For mistakes between FALSE and TRUE classes it is 2, and for all other mistakes it is 1.
INFO : Macro MAE computes MAE for each of the (gold) classes and takes the average.
INFO : Accuracy computes the percentage of correctly predicted classes.
INFO : Macro F1 computes the F1 score for each of the classes and takes their average.
INFO : Macro Recall computes Recall for each of the classes and takes its average.
INFO : Confusion Matrix computes the distribution of predicted classes, where rows are true labels and columns are predicted ones.
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring BOTH the gold and random baseline for task 2 (only for example purposes, scoring multiple files for 1 debate will lead to wrong metrics)
INFO : Started evaluating results for Task 2 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_gold.txt
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_random_baseline.txt
INFO : ======================================================= RESULTS ========================================================
INFO : MEAN ABSOLUTE ERROR:          0.2667
INFO : ========================================================================================================================
INFO : MACRO MEAN ABSOLUTE ERROR:    0.2566
INFO : ========================================================================================================================
INFO : ACCURACY:                     0.8000
INFO : ========================================================================================================================
INFO : MACRO F1:                     0.7928
INFO : ========================================================================================================================
INFO : MACRO RECALL:                 0.8027
INFO : ========================================================================================================================
INFO : CONFUSION MATRIX:             
INFO :                      true          false      half-true
INFO : true                   12              1              3
INFO : false                   3             20              3
INFO : half-true               2              0             16
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN ABSOLUTE ERROR !!!
INFO : Mean Absolute Error (MAE) computes the mean "distance" between the predicted and gold labels.
INFO :   For correct predictions the distance is 0.
INFO :   For mistakes between FALSE and TRUE classes it is 2, and for all other mistakes it is 1.
INFO : Macro MAE computes MAE for each of the (gold) classes and takes the average.
INFO : Accuracy computes the percentage of correctly predicted classes.
INFO : Macro F1 computes the F1 score for each of the classes and takes their average.
INFO : Macro Recall computes Recall for each of the classes and takes its average.
INFO : Confusion Matrix computes the distribution of predicted classes, where rows are true labels and columns are predicted ones.
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
TEST ERROR: Scoring task 2, with predictions that contains a claim_number, which is not present in the gold file.
INFO : Started evaluating results for Task 2 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_other_claim_number.txt
ERROR : No such claim_number: 31 in gold file!
**********
TEST ERROR: Scoring task 2, with predictions that do not contain all claims from the gold file.
INFO : Started evaluating results for Task 2 ...
INFO : Reading gold predictions from file /media/sk/Data/dipl/0code/clef2018-factchecking/data/task2/English/Task2-English-1st-Presidential.txt
INFO : Reading predicted classification labels from file /media/sk/Data/dipl/0code/clef2018-factchecking/scorer/data/task2_not_all_claims.txt
ERROR : The predictions do not match the claims from the gold file - missing or extra claim_number
Traceback (most recent call last):
  File "task2.py", line 248, in <module>
    gold_labels, pred_labels = _read_gold_and_pred(gold_file, pred_file, 'file-{}-claim-number-'.format(idx))
  File "task2.py", line 45, in _read_gold_and_pred
    raise ValueError('The predictions do not match the claims from the gold file - missing or extra claim_number')
ValueError: The predictions do not match the claims from the gold file - missing or extra claim_number